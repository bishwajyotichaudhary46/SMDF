{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Full Stack Data Science\\\\Time Series Analysis\\\\MAJOR PROJECT\\\\SMDF'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Full Stack Data Science\\\\Time Series Analysis\\\\MAJOR PROJECT\\\\SMDF'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    scaler: str\n",
    "    filters: int\n",
    "    kernel_size: int\n",
    "    activation: str\n",
    "    loss: str\n",
    "    optimizer: str\n",
    "    epoch: int\n",
    "    batch_size: int\n",
    "    M01AB: Path\n",
    "    M01AE: Path\n",
    "    N02BA: Path\n",
    "    N02BE: Path\n",
    "    N05B: Path\n",
    "    N05C: Path\n",
    "    R03: Path\n",
    "    R06: Path\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SMDF.constants import *\n",
    "from SMDF.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.hybrid\n",
    "        schema = self.schema.trainer_column\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            filters = params.filters,\n",
    "            kernel_size = params.kernel_size,\n",
    "            scaler = config.scaler,\n",
    "            activation = params.activation,\n",
    "            loss = params.loss,\n",
    "            optimizer = params.optimizer,\n",
    "            epoch=params.epoch,\n",
    "            batch_size=params.batch_size,\n",
    "            M01AB = config.M01AB,\n",
    "            M01AE = config.M01AE,\n",
    "            N02BA = config.N02BA,\n",
    "            N02BE = config.N02BE,\n",
    "            N05B = config.N05B,\n",
    "            N05C = config.N05C,\n",
    "            R03 = config.R03,\n",
    "            R06 = config.R06,\n",
    "           \n",
    "            \n",
    "\n",
    "            \n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from SMDF.logging import logger\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Convolution1D, RepeatVector, TimeDistributed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel:\n",
    "\n",
    "    def __init__(self, config = ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def noramalizing_data(self,column_name):\n",
    "        scaler = StandardScaler()\n",
    "        train = pd.read_csv(self.config.train_data_path)\n",
    "        test = pd.read_csv(self.config.test_data_path)\n",
    "        train.drop(columns=[\"datum\",\"Year\",\"Month\",\"Hour\",\"Weekday Name\"],inplace=True)\n",
    "        test.drop(columns=[\"datum\",\"Year\",\"Month\",\"Hour\",\"Weekday Name\"],inplace=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        #train[columns] = train[columns]\n",
    "        #logger.info(train.head())\n",
    "        train_data = scaler.fit_transform(train[column_name].values.reshape(-1,1))\n",
    "        test_data = scaler.transform(test[column_name].values.reshape(-1,1))\n",
    "\n",
    "        logger.info(train_data.shape)\n",
    "        logger.info(test_data.shape)\n",
    "\n",
    "    \n",
    "        joblib.dump(scaler, os.path.join(self.config.root_dir, self.config.scaler))\n",
    "        \n",
    "        #print(test_data)\n",
    "        return train_data, test_data\n",
    "    # Here we will use previous one 60 days as features and next day as output or target\n",
    "    # Preparing Train dataset\n",
    "\n",
    "    def train_spliting(self,train):\n",
    "        window_size = 60\n",
    "        # Creating a data structure with 60 timesteps and 1 output\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for i in range(window_size, train.shape[0]):\n",
    "            X_train.append(train[i-window_size:i]) \n",
    "            y_train.append(train[i, -1]) # consider Close as target\n",
    "        # Change them to numpy array\n",
    "        X_train, y_train = np.array(X_train).astype('float32'), np.array(y_train).reshape(-1,1)\n",
    "        logger.info(\"Success training data fully spliting\")\n",
    "        return X_train,y_train\n",
    "    \n",
    "    # Preparing Test dataset\n",
    "\n",
    "    def test_spliting(self,train, test):\n",
    "        window_size = 60\n",
    "        # Concatenate train data to test data\n",
    "        dataset_total = np.concatenate((train, test), axis = 0)\n",
    "        # Split test data and last window-size of train data\n",
    "        inputs = dataset_total[len(dataset_total) - len(test) - window_size:]\n",
    "        # Do the same thing for test data\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for i in range(window_size, window_size+len(test)):\n",
    "            X_test.append(inputs[i-window_size:i,:]) \n",
    "            y_test.append(inputs[i,-1]) # consider Close as target\n",
    "        # Change them to numpy array\n",
    "        X_test, y_test = np.array(X_test).astype('float32'), np.array(y_test).reshape(-1, 1)\n",
    "        logger.info(X_test.shape)\n",
    "        return X_test,y_test\n",
    "    \n",
    "    \n",
    "        \n",
    "   \n",
    "    def model_trainer(self,X_train,y_train,key):\n",
    "        columns = [ self.config.M01AB,\n",
    "            self.config.M01AE,\n",
    "            self.config.N02BA,\n",
    "            self.config.N02BE,\n",
    "            self.config.N05B,\n",
    "            self.config.N05C,\n",
    "            self.config.R03,\n",
    "            self.config.R06]\n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(filters=self.config.filters, kernel_size=self.config.kernel_size, activation=self.config.activation, input_shape=(X_train[1,:].shape)))\n",
    "        model.add(Convolution1D(filters=self.config.filters, kernel_size=self.config.kernel_size, activation=self.config.activation))\n",
    "        model.add(Flatten())\n",
    "        model.add(RepeatVector(y_train.shape[1]))\n",
    "        model.add(LSTM(128, activation=self.config.activation, return_sequences=True))\n",
    "        model.add(TimeDistributed(Dense(100, activation=self.config.activation)))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.compile(loss=self.config.loss, optimizer=self.config.optimizer)\n",
    "        logger.info(\"Model Training start\")\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs = self.config.epoch,\n",
    "                    batch_size = self.config.batch_size)\n",
    "        logger.info(\"Model Trained Sucessfully\")\n",
    "        model.summary()\n",
    "        tf.saved_model.save(model, columns[key])\n",
    "        logger.info(\" Model {key} save suceessfully!  \")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 21:22:25,954: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-07-01 21:22:25,959: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-07-01 21:22:25,965: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2023-07-01 21:22:25,968: INFO: common: created directory at: artifacts]\n",
      "[2023-07-01 21:22:25,971: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2023-07-01 21:22:26,005: INFO: 1215740052: (1684, 1)]\n",
      "[2023-07-01 21:22:26,006: INFO: 1215740052: (422, 1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 21:22:26,112: INFO: 1215740052: Success training data fully spliting]\n",
      "[2023-07-01 21:22:26,114: INFO: 1215740052: (422, 60, 1)]\n",
      "[2023-07-01 21:22:27,528: INFO: 1215740052: Model Training start]\n",
      "Epoch 1/150\n",
      "17/17 [==============================] - 7s 246ms/step - loss: 0.9966\n",
      "Epoch 2/150\n",
      "17/17 [==============================] - 3s 185ms/step - loss: 0.9398\n",
      "Epoch 3/150\n",
      "17/17 [==============================] - 3s 188ms/step - loss: 0.8763\n",
      "Epoch 4/150\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 0.8124\n",
      "Epoch 5/150\n",
      "17/17 [==============================] - 3s 182ms/step - loss: 0.7041\n",
      "Epoch 6/150\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 0.5357\n",
      "Epoch 7/150\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.3433\n",
      "Epoch 8/150\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 0.1994\n",
      "Epoch 9/150\n",
      "17/17 [==============================] - 3s 187ms/step - loss: 0.1278\n",
      "Epoch 10/150\n",
      "17/17 [==============================] - 4s 211ms/step - loss: 0.0775\n",
      "Epoch 11/150\n",
      "17/17 [==============================] - 3s 183ms/step - loss: 0.0513\n",
      "Epoch 12/150\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 0.0328\n",
      "Epoch 13/150\n",
      "17/17 [==============================] - 4s 229ms/step - loss: 0.0205\n",
      "Epoch 14/150\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.0132\n",
      "Epoch 15/150\n",
      "17/17 [==============================] - 5s 273ms/step - loss: 0.0106\n",
      "Epoch 16/150\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 0.0104\n",
      "Epoch 17/150\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 0.0178\n",
      "Epoch 18/150\n",
      "17/17 [==============================] - 3s 199ms/step - loss: 0.0134\n",
      "Epoch 19/150\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 0.0097\n",
      "Epoch 20/150\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 0.0069\n",
      "Epoch 21/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.0060\n",
      "Epoch 22/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0062\n",
      "Epoch 23/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0063\n",
      "Epoch 24/150\n",
      "17/17 [==============================] - 3s 163ms/step - loss: 0.0070\n",
      "Epoch 25/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0074\n",
      "Epoch 26/150\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 0.0072\n",
      "Epoch 27/150\n",
      "17/17 [==============================] - 3s 166ms/step - loss: 0.0054\n",
      "Epoch 28/150\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 0.0045\n",
      "Epoch 29/150\n",
      "17/17 [==============================] - 3s 166ms/step - loss: 0.0047\n",
      "Epoch 30/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0054\n",
      "Epoch 31/150\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 0.0053\n",
      "Epoch 32/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0041\n",
      "Epoch 33/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0040\n",
      "Epoch 34/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0035\n",
      "Epoch 35/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0030\n",
      "Epoch 36/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0037\n",
      "Epoch 37/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0036\n",
      "Epoch 38/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0049\n",
      "Epoch 39/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0054\n",
      "Epoch 40/150\n",
      "17/17 [==============================] - 3s 166ms/step - loss: 0.0056\n",
      "Epoch 41/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0062\n",
      "Epoch 42/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0056\n",
      "Epoch 43/150\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 0.0051\n",
      "Epoch 44/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0051\n",
      "Epoch 45/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0061\n",
      "Epoch 46/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0073\n",
      "Epoch 47/150\n",
      "17/17 [==============================] - 3s 174ms/step - loss: 0.0066\n",
      "Epoch 48/150\n",
      "17/17 [==============================] - 3s 184ms/step - loss: 0.0080\n",
      "Epoch 49/150\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 0.0086\n",
      "Epoch 50/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0087\n",
      "Epoch 51/150\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 0.0097\n",
      "Epoch 52/150\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 0.0121\n",
      "Epoch 53/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0135\n",
      "Epoch 54/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0138\n",
      "Epoch 55/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0145\n",
      "Epoch 56/150\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 0.0130\n",
      "Epoch 57/150\n",
      "17/17 [==============================] - 3s 177ms/step - loss: 0.0114\n",
      "Epoch 58/150\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 0.0110\n",
      "Epoch 59/150\n",
      "17/17 [==============================] - 3s 181ms/step - loss: 0.0157\n",
      "Epoch 60/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0156\n",
      "Epoch 61/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.0140\n",
      "Epoch 62/150\n",
      "17/17 [==============================] - 3s 175ms/step - loss: 0.0115\n",
      "Epoch 63/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0092\n",
      "Epoch 64/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 0.0072\n",
      "Epoch 65/150\n",
      "17/17 [==============================] - 3s 191ms/step - loss: 0.0066\n",
      "Epoch 66/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0056\n",
      "Epoch 67/150\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 0.0068\n",
      "Epoch 68/150\n",
      "17/17 [==============================] - 3s 199ms/step - loss: 0.0059\n",
      "Epoch 69/150\n",
      "17/17 [==============================] - 3s 179ms/step - loss: 0.0043\n",
      "Epoch 70/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.0029\n",
      "Epoch 71/150\n",
      "17/17 [==============================] - 3s 184ms/step - loss: 0.0027\n",
      "Epoch 72/150\n",
      "17/17 [==============================] - 3s 191ms/step - loss: 0.0029\n",
      "Epoch 73/150\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 0.0027\n",
      "Epoch 74/150\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 0.0024\n",
      "Epoch 75/150\n",
      "17/17 [==============================] - 4s 237ms/step - loss: 0.0018\n",
      "Epoch 76/150\n",
      "17/17 [==============================] - 4s 228ms/step - loss: 0.0020\n",
      "Epoch 77/150\n",
      "17/17 [==============================] - 4s 220ms/step - loss: 0.0020\n",
      "Epoch 78/150\n",
      "17/17 [==============================] - 4s 222ms/step - loss: 0.0018\n",
      "Epoch 79/150\n",
      "17/17 [==============================] - 4s 234ms/step - loss: 0.0019\n",
      "Epoch 80/150\n",
      "17/17 [==============================] - 3s 200ms/step - loss: 0.0017\n",
      "Epoch 81/150\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.0018\n",
      "Epoch 82/150\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 0.0014\n",
      "Epoch 83/150\n",
      "17/17 [==============================] - 3s 189ms/step - loss: 0.0011\n",
      "Epoch 84/150\n",
      "17/17 [==============================] - 4s 232ms/step - loss: 8.7144e-04\n",
      "Epoch 85/150\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 8.7210e-04\n",
      "Epoch 86/150\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 6.9542e-04\n",
      "Epoch 87/150\n",
      "17/17 [==============================] - 3s 200ms/step - loss: 7.3399e-04\n",
      "Epoch 88/150\n",
      "17/17 [==============================] - 3s 166ms/step - loss: 7.9920e-04\n",
      "Epoch 89/150\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 8.0538e-04\n",
      "Epoch 90/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 8.5757e-04\n",
      "Epoch 91/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 9.8279e-04\n",
      "Epoch 92/150\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 9.8512e-04\n",
      "Epoch 93/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 0.0011\n",
      "Epoch 94/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0012\n",
      "Epoch 95/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0014\n",
      "Epoch 96/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.0016\n",
      "Epoch 97/150\n",
      "17/17 [==============================] - 3s 192ms/step - loss: 0.0017\n",
      "Epoch 98/150\n",
      "17/17 [==============================] - 3s 187ms/step - loss: 0.0023\n",
      "Epoch 99/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0024\n",
      "Epoch 100/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0032\n",
      "Epoch 101/150\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 0.0060\n",
      "Epoch 102/150\n",
      "17/17 [==============================] - 3s 189ms/step - loss: 0.0080\n",
      "Epoch 103/150\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 0.0105\n",
      "Epoch 104/150\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 0.0123\n",
      "Epoch 105/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0210\n",
      "Epoch 106/150\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 0.0234\n",
      "Epoch 107/150\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 0.0230\n",
      "Epoch 108/150\n",
      "17/17 [==============================] - 4s 227ms/step - loss: 0.0184\n",
      "Epoch 109/150\n",
      "17/17 [==============================] - 3s 182ms/step - loss: 0.0164\n",
      "Epoch 110/150\n",
      "17/17 [==============================] - 3s 189ms/step - loss: 0.0148\n",
      "Epoch 111/150\n",
      "17/17 [==============================] - 3s 189ms/step - loss: 0.0128\n",
      "Epoch 112/150\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.0093\n",
      "Epoch 113/150\n",
      "17/17 [==============================] - 3s 195ms/step - loss: 0.0082\n",
      "Epoch 114/150\n",
      "17/17 [==============================] - 4s 211ms/step - loss: 0.0074\n",
      "Epoch 115/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0051\n",
      "Epoch 116/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0050\n",
      "Epoch 117/150\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 0.0060\n",
      "Epoch 118/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 0.0060\n",
      "Epoch 119/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 0.0044\n",
      "Epoch 120/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0046\n",
      "Epoch 121/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0049\n",
      "Epoch 122/150\n",
      "17/17 [==============================] - 3s 189ms/step - loss: 0.0039\n",
      "Epoch 123/150\n",
      "17/17 [==============================] - 3s 186ms/step - loss: 0.0035\n",
      "Epoch 124/150\n",
      "17/17 [==============================] - 3s 181ms/step - loss: 0.0027\n",
      "Epoch 125/150\n",
      "17/17 [==============================] - 3s 206ms/step - loss: 0.0021\n",
      "Epoch 126/150\n",
      "17/17 [==============================] - 3s 192ms/step - loss: 0.0025\n",
      "Epoch 127/150\n",
      "17/17 [==============================] - 3s 182ms/step - loss: 0.0027\n",
      "Epoch 128/150\n",
      "17/17 [==============================] - 3s 185ms/step - loss: 0.0029\n",
      "Epoch 129/150\n",
      "17/17 [==============================] - 3s 174ms/step - loss: 0.0025\n",
      "Epoch 130/150\n",
      "17/17 [==============================] - 3s 179ms/step - loss: 0.0021\n",
      "Epoch 131/150\n",
      "17/17 [==============================] - 3s 181ms/step - loss: 0.0024\n",
      "Epoch 132/150\n",
      "17/17 [==============================] - 3s 181ms/step - loss: 0.0026\n",
      "Epoch 133/150\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 0.0031\n",
      "Epoch 134/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.0025\n",
      "Epoch 135/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 0.0016\n",
      "Epoch 136/150\n",
      "17/17 [==============================] - 3s 179ms/step - loss: 0.0013\n",
      "Epoch 137/150\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 8.1242e-04\n",
      "Epoch 138/150\n",
      "17/17 [==============================] - 3s 183ms/step - loss: 6.1599e-04\n",
      "Epoch 139/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 4.6081e-04\n",
      "Epoch 140/150\n",
      "17/17 [==============================] - 3s 188ms/step - loss: 5.0396e-04\n",
      "Epoch 141/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 5.8135e-04\n",
      "Epoch 142/150\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 6.2809e-04\n",
      "Epoch 143/150\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 6.4382e-04\n",
      "Epoch 144/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 5.7945e-04\n",
      "Epoch 145/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 8.5217e-04\n",
      "Epoch 146/150\n",
      "17/17 [==============================] - 3s 167ms/step - loss: 9.3773e-04\n",
      "Epoch 147/150\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 0.0014\n",
      "Epoch 148/150\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.0014\n",
      "Epoch 149/150\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.0015\n",
      "Epoch 150/150\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 0.0015\n",
      "[2023-07-01 21:30:18,491: INFO: 1215740052: Model Trained Sucessfully]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 58, 256)           1024      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 56, 256)           196864    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 14336)             0         \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 1, 14336)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 128)            7406080   \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 1, 100)           12900     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 1, 1)             101       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,616,969\n",
      "Trainable params: 7,616,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[2023-07-01 21:30:20,845: WARNING: save: Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.]\n",
      "[2023-07-01 21:30:22,864: INFO: builder_impl: Assets written to: artifacts/model_trainer/M01AB\\assets]\n",
      "[2023-07-01 21:30:23,152: INFO: 1215740052:  Model {key} save suceessfully!  ]\n",
      "[2023-07-01 21:30:23,167: INFO: 1215740052: (1684, 1)]\n",
      "[2023-07-01 21:30:23,169: INFO: 1215740052: (422, 1)]\n",
      "[2023-07-01 21:30:23,179: INFO: 1215740052: Success training data fully spliting]\n",
      "[2023-07-01 21:30:23,182: INFO: 1215740052: (422, 60, 1)]\n",
      "[2023-07-01 21:30:23,474: INFO: 1215740052: Model Training start]\n",
      "Epoch 1/150\n",
      "17/17 [==============================] - 5s 179ms/step - loss: 0.9924\n",
      "Epoch 2/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 0.9620\n",
      "Epoch 3/150\n",
      "17/17 [==============================] - 3s 186ms/step - loss: 0.9076\n",
      "Epoch 4/150\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 0.8406\n",
      "Epoch 5/150\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 0.7305\n",
      "Epoch 6/150\n",
      "17/17 [==============================] - 3s 188ms/step - loss: 0.5872\n",
      "Epoch 7/150\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 0.3856\n",
      "Epoch 8/150\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 0.2147\n",
      "Epoch 9/150\n",
      "17/17 [==============================] - 3s 191ms/step - loss: 0.1115\n",
      "Epoch 10/150\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 0.0677\n",
      "Epoch 11/150\n",
      "17/17 [==============================] - 4s 233ms/step - loss: 0.0426\n",
      "Epoch 12/150\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 0.0304\n",
      "Epoch 13/150\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 0.0215\n",
      "Epoch 14/150\n",
      "17/17 [==============================] - 3s 177ms/step - loss: 0.0189\n",
      "Epoch 15/150\n",
      "17/17 [==============================] - 3s 183ms/step - loss: 0.0145\n",
      "Epoch 16/150\n",
      "17/17 [==============================] - 3s 189ms/step - loss: 0.0104\n",
      "Epoch 17/150\n",
      "17/17 [==============================] - 3s 175ms/step - loss: 0.0079\n",
      "Epoch 18/150\n",
      "17/17 [==============================] - 3s 191ms/step - loss: 0.0079\n",
      "Epoch 19/150\n",
      "17/17 [==============================] - 3s 184ms/step - loss: 0.0060\n",
      "Epoch 20/150\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 0.0047\n",
      "Epoch 21/150\n",
      "17/17 [==============================] - 3s 180ms/step - loss: 0.0060\n",
      "Epoch 22/150\n",
      "17/17 [==============================] - 3s 188ms/step - loss: 0.0059\n",
      "Epoch 23/150\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 0.0054\n",
      "Epoch 24/150\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 0.0048\n",
      "Epoch 25/150\n",
      "17/17 [==============================] - 3s 184ms/step - loss: 0.0050\n",
      "Epoch 26/150\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 0.0056\n",
      "Epoch 27/150\n",
      "17/17 [==============================] - 3s 170ms/step - loss: 0.0068\n",
      "Epoch 28/150\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 0.0070\n",
      "Epoch 29/150\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 0.0073\n",
      "Epoch 30/150\n",
      "17/17 [==============================] - 4s 223ms/step - loss: 0.0097\n",
      "Epoch 31/150\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.0242\n",
      "Epoch 32/150\n",
      "17/17 [==============================] - 4s 237ms/step - loss: 0.0230\n",
      "Epoch 33/150\n",
      "17/17 [==============================] - 4s 254ms/step - loss: 0.0190\n",
      "Epoch 34/150\n",
      "17/17 [==============================] - 4s 239ms/step - loss: 0.0132\n",
      "Epoch 35/150\n",
      "17/17 [==============================] - 4s 232ms/step - loss: 0.0105\n",
      "Epoch 36/150\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.0100"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m         X_train,y_train \u001b[39m=\u001b[39m  trainer\u001b[39m.\u001b[39mtrain_spliting(train)\n\u001b[0;32m     16\u001b[0m         X_test, y_test \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest_spliting(train,test)\n\u001b[1;32m---> 17\u001b[0m         model \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mmodel_trainer(X_train,y_train,key)\n\u001b[0;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     20\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[24], line 85\u001b[0m, in \u001b[0;36mHybridModel.model_trainer\u001b[1;34m(self, X_train, y_train, key)\u001b[0m\n\u001b[0;32m     83\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mloss, optimizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moptimizer)\n\u001b[0;32m     84\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mModel Training start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, \n\u001b[0;32m     86\u001b[0m           epochs \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mepoch,\n\u001b[0;32m     87\u001b[0m             batch_size \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbatch_size)\n\u001b[0;32m     88\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mModel Trained Sucessfully\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\\smdf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_config = config.get_model_trainer_config()\n",
    "    trainer = HybridModel(config=data_config)\n",
    "    columns = [\"M01AB\",\n",
    "            \"M01AE\",\n",
    "            \"N02BA\",\n",
    "            \"N02BE\",\n",
    "            \"N05B\",\n",
    "            \"N05C\",\n",
    "            \"R03\",\n",
    "            \"R06\"]\n",
    "    for key,val in enumerate(columns):\n",
    "        train, test = trainer.noramalizing_data(val)\n",
    "        X_train,y_train =  trainer.train_spliting(train)\n",
    "        X_test, y_test = trainer.test_spliting(train,test)\n",
    "        model = trainer.model_trainer(X_train,y_train,key)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
