{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from SMDF.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Full Stack Data Science\\\\Time Series Analysis\\\\MAJOR PROJECT\\\\SMDF\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Full Stack Data Science\\Time Series Analysis\\MAJOR PROJECT\\SMDF\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingPipeline:\n",
    "    def __init__(self):\n",
    "        self.M01AB = tf.saved_model.load(Path(\"artifacts/model_trainer/M01AB\"))\n",
    "        self.M01AE = tf.saved_model.load(\"artifacts/model_trainer/M01AE\")\n",
    "        self.N02BA = tf.saved_model.load(\"artifacts/model_trainer/N02BA\")\n",
    "        self.N02BE = tf.saved_model.load(\"artifacts/model_trainer/N02BE\")\n",
    "        self.N05B = tf.saved_model.load(\"artifacts/model_trainer/N05B\")\n",
    "        self.N05C = tf.saved_model.load(\"artifacts/model_trainer/N05C\")\n",
    "        self.R03 = tf.saved_model.load(\"artifacts/model_trainer/R03\")\n",
    "        self.R06 = tf.saved_model.load(\"artifacts/model_trainer/R06\")\n",
    "        self.train = pd.read_csv(\"artifacts/data_spliting/train.csv\")\n",
    "        self.test = pd.read_csv(\"artifacts/data_spliting/test.csv\")\n",
    "        self.scaler = joblib\n",
    "\n",
    "    def noramalizing_data(self,column_name):\n",
    "        scaler = StandardScaler()\n",
    "        #self.train.drop(columns=[\"datum\",\"Year\",\"Month\",\"Hour\",\"Weekday Name\"],inplace=True)\n",
    "        #self.test.drop(columns=[\"datum\",\"Year\",\"Month\",\"Hour\",\"Weekday Name\"],inplace=True)\n",
    "\n",
    "        train_data = scaler.fit_transform(self.train[column_name].values.reshape(-1,1))\n",
    "\n",
    "        test_data = scaler.transform(self.test[column_name].values.reshape(-1,1))\n",
    "\n",
    "        \n",
    "        logger.info(test_data.shape)\n",
    "        \n",
    "        #print(test_data)\n",
    "        return  test_data,train_data\n",
    "    \n",
    "    def test_spliting(self,column_name):\n",
    "        train, test = self.noramalizing_data(column_name)\n",
    "        window_size = 60\n",
    "        # Concatenate train data to test data\n",
    "        dataset_total = np.concatenate((train, test), axis = 0)\n",
    "        # Split test data and last window-size of train data\n",
    "        inputs = dataset_total[len(dataset_total) - len(test) - window_size:]\n",
    "        # Do the same thing for test data\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        for i in range(window_size, window_size+len(test)):\n",
    "            X_test.append(inputs[i-window_size:i,:]) \n",
    "            y_test.append(inputs[i,-1]) # consider Close as target\n",
    "        # Change them to numpy array\n",
    "        X_test, y_test = np.array(X_test).astype('float32'), np.array(y_test).reshape(-1, 1)\n",
    "        logger.info(X_test.shape)\n",
    "    \n",
    "        return X_test,y_test\n",
    "\n",
    "\n",
    "    def helper_forecast(self,column_name,key):\n",
    "        X_test, _ = self.test_spliting(column_name)\n",
    "        \n",
    "        logger.info(\"Forecasting Start \")\n",
    "        logger.info(X_test.shape)\n",
    "        models = [self.M01AB,\n",
    "                  self.M01AE,\n",
    "                  self.N02BA,\n",
    "                  self.N02BE,\n",
    "                  self.N05B,\n",
    "                  self.N05C,\n",
    "                  self.R03,\n",
    "                  self.R06]\n",
    "        time_distribution = ['time_distributed_1',\n",
    "                             'time_distributed_3',\n",
    "                             'time_distributed_5',\n",
    "                             'time_distributed_7',\n",
    "                             'time_distributed_9',\n",
    "                             'time_distributed_11',\n",
    "                             'time_distributed_13',\n",
    "                             'time_distributed_15']\n",
    "        \n",
    "        x_input=X_test[-1:].reshape(1,-1)\n",
    "        \n",
    "        \n",
    "        temp_input=list(x_input)\n",
    "        temp_input=temp_input[0].tolist()\n",
    "        logger.info(len(temp_input))\n",
    "        \n",
    "    \n",
    "        lst_output = []\n",
    "        n_step = 60\n",
    "        i = 0\n",
    "        while i<30:\n",
    "        \n",
    "            if len(temp_input) >60:\n",
    "                x_input=np.array(temp_input[1:])\n",
    "                x_input=x_input.reshape(1,-1)\n",
    "                x_input = x_input.reshape((1, n_step, 1))\n",
    "                #print(x_input)\n",
    "                #forecast.append(x_input)\n",
    "                #print(forecast)\n",
    "                #logger.info(\"60 >\")\n",
    "                x_input = tf.constant(x_input, dtype=tf.float32)\n",
    "                model = models[key].signatures[\"serving_default\"]\n",
    "                test_predict = model(x_input)\n",
    "                #logger.info(test_predict)\n",
    "                #print(test_predict)\n",
    "                predictions = test_predict[time_distribution[key]].numpy()\n",
    "                temp_input=temp_input[1:]\n",
    "                lst_output.extend(predictions[0][0].tolist())\n",
    "                i=i+1\n",
    "\n",
    "            else:\n",
    "                x_input = np.array(x_input)\n",
    "                x_input = x_input.reshape((1, n_step,1))\n",
    "                \n",
    "                #x_input = x_input.astype(float) \n",
    "                model = models[key].signatures[\"serving_default\"]\n",
    "                test_predict = model(tf.constant(x_input))\n",
    "                #logger.info(test_predict)\n",
    "                #print(test_predict)\n",
    "                predictions = test_predict[time_distribution[key]].numpy()\n",
    "                #yhat = model.predict(x_input, verbose=0)\n",
    "                temp_input.extend(predictions[0][0].tolist())\n",
    "                lst_output.extend(predictions[0][0].tolist())\n",
    "                i = i+1\n",
    "                #logger.info(type(temp_input[0]))\n",
    "                    \n",
    "       \n",
    "\n",
    "        \n",
    "        return lst_output\n",
    "    def forecasting(self):\n",
    "        \n",
    "        today = datetime.datetime.now().strftime('%m/%d/%Y')\n",
    "        forecast_dates=pd.date_range(start=today, periods = 30)\n",
    "        forecast=pd.DataFrame(forecast_dates,columns=['Date'])\n",
    "        forecast['Date']=pd.to_datetime(forecast['Date'])\n",
    "        forecast.sort_values('Date',inplace=True)\n",
    "        columns = [\"M01AB\",\n",
    "            \"M01AE\",\n",
    "            \"N02BA\",\n",
    "            \"N02BE\",\n",
    "            \"N05B\",\n",
    "            \"N05C\",\n",
    "            \"R03\",\n",
    "            \"R06\"]\n",
    "        for i,val in enumerate(columns):\n",
    "            forecast_data = self.helper_forecast(val,i)\n",
    "            #forecast_data=self.scaler.inverse_transform(np.array(forecas_data).reshape(-1, 1))\n",
    "            forecast[val] = forecast_data\n",
    "            \n",
    "        return forecast.to_csv(\"artifacts/forecast_data/forecast.csv\",index=False),forecast\n",
    "    \n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-02 21:02:50,604: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:50,612: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:50,612: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:50,620: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:50,620: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:51,069: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:51,086: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:51,094: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:51,102: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:51,110: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:51,470: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:51,478: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:51,478: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:51,486: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:51,486: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:51,837: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:51,861: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:51,861: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:51,870: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:51,870: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:52,238: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:52,245: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:52,245: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:52,245: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:52,253: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:52,587: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:52,595: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:52,595: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:52,595: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:52,595: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:52,952: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:52,960: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:52,960: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:52,960: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:52,960: INFO: 1444572149: 60]\n",
      "[2023-07-02 21:02:53,327: INFO: 1444572149: (422, 1)]\n",
      "[2023-07-02 21:02:53,336: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:53,336: INFO: 1444572149: Forecasting Start ]\n",
      "[2023-07-02 21:02:53,336: INFO: 1444572149: (1684, 60, 1)]\n",
      "[2023-07-02 21:02:53,336: INFO: 1444572149: 60]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "          Date     M01AB     M01AE     N02BA     N02BE      N05B      N05C  \\\n",
       " 0  2023-07-02  2.223779  0.535053  0.847471 -0.885573 -0.489597 -0.695162   \n",
       " 1  2023-07-03  1.180167  0.249070  0.230377 -0.910907  1.368709 -0.419947   \n",
       " 2  2023-07-04  1.180167  0.249070  0.230377 -0.910907  1.368709 -0.419947   \n",
       " 3  2023-07-05  0.232661 -0.123547 -0.298821 -0.653181 -0.456261  0.012610   \n",
       " 4  2023-07-06  0.232661 -0.123547 -0.298821 -0.653181 -0.456261  0.012610   \n",
       " 5  2023-07-07 -1.560967 -0.962537 -0.447988 -0.647456 -0.025767 -0.026324   \n",
       " 6  2023-07-08 -1.560967 -0.962537 -0.447988 -0.647456 -0.025767 -0.026324   \n",
       " 7  2023-07-09  1.078282 -0.279994 -1.101143 -0.272654 -0.460058  0.503143   \n",
       " 8  2023-07-10  1.078282 -0.279994 -1.101143 -0.272654 -0.460058  0.503143   \n",
       " 9  2023-07-11 -0.175109 -0.120284 -0.704017 -0.441894 -0.344218 -0.440330   \n",
       " 10 2023-07-12 -0.175109 -0.120284 -0.704017 -0.441894 -0.344218 -0.440330   \n",
       " 11 2023-07-13 -0.568018  1.269271 -1.486926 -0.339246 -0.094896 -0.130663   \n",
       " 12 2023-07-14 -0.568018  1.269271 -1.486926 -0.339246 -0.094896 -0.130663   \n",
       " 13 2023-07-15  1.097815 -0.014119 -0.640556 -0.522297 -0.071981  0.082784   \n",
       " 14 2023-07-16  1.097815 -0.014119 -0.640556 -0.522297 -0.071981  0.082784   \n",
       " 15 2023-07-17 -0.107179 -0.620522 -0.760741 -0.228570  0.537520  1.576575   \n",
       " 16 2023-07-18 -0.107179 -0.620522 -0.760741 -0.228570  0.537520  1.576575   \n",
       " 17 2023-07-19  0.156935  0.589394 -0.023100 -0.592780 -0.412726 -0.815645   \n",
       " 18 2023-07-20  0.156935  0.589394 -0.023100 -0.592780 -0.412726 -0.815645   \n",
       " 19 2023-07-21 -0.452927 -0.179324 -0.074807 -0.866048  0.371530 -0.337108   \n",
       " 20 2023-07-22 -0.452927 -0.179324 -0.074807 -0.866048  0.371530 -0.337108   \n",
       " 21 2023-07-23 -0.219143  0.120908 -0.414180 -0.290296 -0.342816 -0.236337   \n",
       " 22 2023-07-24 -0.219143  0.120908 -0.414180 -0.290296 -0.342816 -0.236337   \n",
       " 23 2023-07-25 -0.286066  0.189928 -0.245377 -0.410388 -0.455243  0.883038   \n",
       " 24 2023-07-26 -0.286066  0.189928 -0.245377 -0.410388 -0.455243  0.883038   \n",
       " 25 2023-07-27  0.627439 -0.749977  0.364794 -0.506759 -0.273728 -0.062586   \n",
       " 26 2023-07-28  0.627439 -0.749977  0.364794 -0.506759 -0.273728 -0.062586   \n",
       " 27 2023-07-29  0.425031  1.235860  0.324461 -0.477942  0.290994  0.803983   \n",
       " 28 2023-07-30  0.425031  1.235860  0.324461 -0.477942  0.290994  0.803983   \n",
       " 29 2023-07-31  0.842433  0.599278 -0.635702 -0.142394  0.387863 -0.199889   \n",
       " \n",
       "          R03       R06  \n",
       " 0  -0.748445 -0.334251  \n",
       " 1  -0.692923  1.062894  \n",
       " 2  -0.692923  1.062894  \n",
       " 3  -0.419734  0.688361  \n",
       " 4  -0.419734  0.688361  \n",
       " 5  -0.251113  0.870275  \n",
       " 6  -0.251113  0.870275  \n",
       " 7  -0.652085 -0.056136  \n",
       " 8  -0.652085 -0.056136  \n",
       " 9  -0.332261  0.642116  \n",
       " 10 -0.332261  0.642116  \n",
       " 11 -0.491190  1.684599  \n",
       " 12 -0.491190  1.684599  \n",
       " 13 -0.565912 -0.389783  \n",
       " 14 -0.565912 -0.389783  \n",
       " 15 -0.688180  0.441989  \n",
       " 16 -0.688180  0.441989  \n",
       " 17 -0.236364  0.658260  \n",
       " 18 -0.236364  0.658260  \n",
       " 19 -0.614830  0.085537  \n",
       " 20 -0.614830  0.085537  \n",
       " 21 -0.634212 -0.099567  \n",
       " 22 -0.634212 -0.099567  \n",
       " 23 -0.441122  0.738695  \n",
       " 24 -0.441122  0.738695  \n",
       " 25 -0.709751  0.326609  \n",
       " 26 -0.709751  0.326609  \n",
       " 27 -1.013710 -0.106699  \n",
       " 28 -1.013710 -0.106699  \n",
       " 29 -0.491405  0.555702  )"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_cls = ForecastingPipeline()\n",
    "forecast_cls.forecasting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
